{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4f7249c",
   "metadata": {},
   "source": [
    "## Traffic Light Detection using FasterRCNN\n",
    "In this notebook, I will be walking through the source code of the traffic light detection algorithm. In summary, the implemented model starts with the FasterRCNN architecture pre-trained on the COCO dataset. This model is then fine-tuned based on the LISA Traffic Light Dataset which contains 44 minutes of annotated traffic light datac collected in San Diego, California."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d9c434",
   "metadata": {},
   "source": [
    "### 1. Importing Necessary Libraries\n",
    "Importing necessary libraries and packages for this project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7e791d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "def install_and_import_packages(package_list):\n",
    "    for p in package_list:\n",
    "        # Install package with pip3 if not already installed\n",
    "        if importlib.util.find_spec(p) is None:\n",
    "            print(f\"{p} is not installed. Installing ...\")\n",
    "            try:\n",
    "                !pip3 install {p\n",
    "                print(f\"{p} is now installed and imported.\")\n",
    "            except ImportError as e:\n",
    "                print(f\"Failed to import {p}: {e}\")\n",
    "        else:\n",
    "            print(f\"Package exists: {p}\")\n",
    "                               \n",
    "        # Equivalent to \"import package\"                       \n",
    "        globals()[p] = importlib.import_module(p)\n",
    "    print(\"All packages successfully installed and imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8c541eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu117\n",
      "Requirement already satisfied: torch in c:\\users\\brand\\anaconda3\\envs\\envcuda\\lib\\site-packages (2.0.1+cu117)\n",
      "Requirement already satisfied: torchvision in c:\\users\\brand\\anaconda3\\envs\\envcuda\\lib\\site-packages (0.15.2+cu117)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\brand\\anaconda3\\envs\\envcuda\\lib\\site-packages (2.0.2+cu117)\n",
      "Requirement already satisfied: filelock in c:\\users\\brand\\anaconda3\\envs\\envcuda\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\brand\\anaconda3\\envs\\envcuda\\lib\\site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\brand\\anaconda3\\envs\\envcuda\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\brand\\anaconda3\\envs\\envcuda\\lib\\site-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\brand\\anaconda3\\envs\\envcuda\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\brand\\anaconda3\\envs\\envcuda\\lib\\site-packages (from torchvision) (1.24.1)\n",
      "Requirement already satisfied: requests in c:\\users\\brand\\anaconda3\\envs\\envcuda\\lib\\site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\brand\\anaconda3\\envs\\envcuda\\lib\\site-packages (from torchvision) (9.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\brand\\anaconda3\\envs\\envcuda\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\brand\\anaconda3\\envs\\envcuda\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\brand\\anaconda3\\envs\\envcuda\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\brand\\anaconda3\\envs\\envcuda\\lib\\site-packages (from requests->torchvision) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\brand\\anaconda3\\envs\\envcuda\\lib\\site-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\brand\\anaconda3\\envs\\envcuda\\lib\\site-packages (from sympy->torch) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "# Installing the correct version of torch for cuda on windows 11\n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "60ef9530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package exists: albumentations\n",
      "Package exists: cv2\n",
      "Package exists: datetime\n",
      "Package exists: fastai\n",
      "Package exists: matplotlib\n",
      "Package exists: numpy\n",
      "Package exists: os\n",
      "Package exists: pynvml\n",
      "Package exists: pandas\n",
      "Package exists: seaborn\n",
      "Package exists: time\n",
      "Package exists: torch\n",
      "Package exists: torchvision\n",
      "Package exists: tqdm\n",
      "Package exists: warnings\n",
      "All packages successfully installed and imported.\n"
     ]
    }
   ],
   "source": [
    "package_list = [\n",
    "    'albumentations',\n",
    "    'cv2', \n",
    "    'datetime',\n",
    "    'fastai',\n",
    "    'matplotlib',\n",
    "    'numpy', \n",
    "    'os', \n",
    "    'pynvml', \n",
    "    'pandas',\n",
    "    'seaborn',\n",
    "    'time',\n",
    "    'torch',\n",
    "    'torchvision',\n",
    "    'tqdm',\n",
    "    'warnings'\n",
    "]\n",
    "\n",
    "install_and_import_packages(package_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea6b8050",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "477ebb10",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# General\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "\n",
    "# Replace with fastai library\n",
    "from sklearn.model_selection import train_test_split, GroupKFold, StratifiedKFold\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.ops import nms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SequentialSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13667b95",
   "metadata": {},
   "source": [
    "Here, we create a seed for reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaa0f61",
   "metadata": {},
   "source": [
    "### 2. Load Data\n",
    "The [LISA Traffic Light Dataset](https://www.kaggle.com/datasets/mbornoe/lisa-traffic-light-dataset) is organized into multiple files each with their respective annotation files. The dataset is structure as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4957aff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Annotations',\n",
       " 'daySequence1',\n",
       " 'daySequence2',\n",
       " 'dayTrain',\n",
       " 'nightSequence1',\n",
       " 'nightSequence2',\n",
       " 'nightTrain',\n",
       " 'sample-dayClip6',\n",
       " 'sample-nightClip1']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'archive'\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79db238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DAY_TRAIN_PATH = 'archive/Annotations/Annotations/dayTrain/'\n",
    "NIGHT_TRAIN_PATH = 'archive/Annotations/Annotations/nightTrain/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a2002a",
   "metadata": {},
   "source": [
    "#### Merge all different annotation files into a single file\n",
    "Here we will be merging all the data into one dataframe. \n",
    "\n",
    "*Note: we will also add the \"isNight\" feature to split the data such that there is a balance of day and night clips in both the train and test sets.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30cbb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_day = []\n",
    "for clipName in tqdm(sorted(os.listdir(DAY_TRAIN_PATH))):\n",
    "    if 'dayClip' not in clipName:\n",
    "        continue\n",
    "    df = pd.read_csv(os.path.join(DAY_TRAIN_PATH,clipName,'frameAnnotationsBOX.csv'),sep=';')\n",
    "    train_day.append(df)\n",
    "    \n",
    "train_day_df = pd.concat(train_day,axis=0)\n",
    "train_day_df['isNight'] = 0\n",
    "    \n",
    "train_night = []\n",
    "for clipName in tqdm(sorted(os.listdir(NIGHT_TRAIN_PATH))):\n",
    "    if 'nightClip' not in clipName:\n",
    "        continue\n",
    "    df = pd.read_csv(os.path.join(NIGHT_TRAIN_PATH,clipName,'frameAnnotationsBOX.csv'),sep=';')\n",
    "    train_night.append(df)\n",
    "\n",
    "train_night_df = pd.concat(train_night,axis=0)\n",
    "train_night_df['isNight'] = 1\n",
    "\n",
    "df = pd.concat([train_day_df,train_night_df],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70bc5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6172ee4",
   "metadata": {},
   "source": [
    "### 3. Data Preprocessing\n",
    "Here we preprocess the data such that it is easier to work with. We will delete duplicate columns, change the \"Filename\" column in the dataframe to the full path of the image file, and simplify the annotations to only stop (RED), go (GREEN), and warning (YELLOW)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17b5b84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Duplicate Columns\n",
    "np.all(df['Origin file'] == df['Origin track']), np.all(df['Origin frame number'] == df['Origin track frame number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d02bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droppin duplicate columns & \"Origin file\" as we don't need it\n",
    "df = df.drop(['Origin file','Origin track','Origin track frame number'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a397f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here Filename (Location of Image) is different -> Change it to appropriate name\n",
    "# Ex. dayTraining/dayClip1--00000.jpg -> dayTrain/dayTrain/dayClip1/frames/dayClip1--00000.jpg\n",
    "\n",
    "def changeFilename(x):\n",
    "    filename = x.Filename\n",
    "    isNight = x.isNight\n",
    "    \n",
    "    splitted = filename.split('/')\n",
    "    clipName = splitted[-1].split('--')[0]\n",
    "    if isNight:\n",
    "        return os.path.join(DATA_PATH,f'nightTrain/nightTrain/{clipName}/frames/{splitted[-1]}')\n",
    "    else:\n",
    "        return os.path.join(DATA_PATH,f'dayTrain/dayTrain/{clipName}/frames/{splitted[-1]}')\n",
    "\n",
    "df['Filename'] = df.apply(changeFilename,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baa2446",
   "metadata": {},
   "source": [
    "The current dataframe is populated with the following annotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a59797",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Annotation tag'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad758c5",
   "metadata": {},
   "source": [
    "Simplifying the annotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1891d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will change annotations to only -> stop (RED), go (GREEN) & warning (YELLOW)\n",
    "label_to_idx = {'go':1, 'warning':2, 'stop': 3}\n",
    "idx_to_label = {v:k for k,v in label_to_idx.items()}\n",
    "\n",
    "def changeAnnotation(x):\n",
    "    if 'go' in x['Annotation tag']:\n",
    "        return label_to_idx['go']\n",
    "    elif 'warning' in x['Annotation tag']:\n",
    "        return label_to_idx['warning']\n",
    "    elif 'stop' in x['Annotation tag']:\n",
    "        return label_to_idx['stop']\n",
    "    \n",
    "df['Annotation tag'] = df.apply(changeAnnotation,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfb3d02",
   "metadata": {},
   "source": [
    "The annotation tags are now represented by an integer value. 1 for 'go', 2 for 'warning', and '3' for stop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5bb71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_tags = df['Annotation tag'].unique()\n",
    "annotation_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2319b8f8",
   "metadata": {},
   "source": [
    "Let's shorten the column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecb931c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing Column Names\n",
    "df.columns = ['image_id','label','x_min','y_min','x_max','y_max','frame','isNight']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b61233",
   "metadata": {},
   "source": [
    "Let's take a look at our preprocessed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba05c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06535ce",
   "metadata": {},
   "source": [
    "Let's also take a look at some of the data in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7b6e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(len(annotation_tags),1,figsize=(15,10*len(annotation_tags)))\n",
    "\n",
    "for i, tag in enumerate(annotation_tags):\n",
    "    sample = df[df['label']==tag].sample(1)\n",
    "    bbox = sample[['x_min','y_min','x_max','y_max']].values[0]\n",
    "    \n",
    "    image = cv2.imread(sample.image_id.values[0])\n",
    "    image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    cv2.rectangle(image,(bbox[0],bbox[1]),(bbox[2],bbox[3]),(220, 0, 0), 2)\n",
    "    \n",
    "    ax[i].set_title(idx_to_label[tag])\n",
    "    ax[i].set_axis_off()\n",
    "    ax[i].imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5452c644",
   "metadata": {},
   "source": [
    "Lastly, let's get an idea of how many unique images exist in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281e4951",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of Unique Images: \",df.image_id.nunique(),'/',df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09b683b",
   "metadata": {},
   "source": [
    "#### Validation Scheme\n",
    "Since we have video clips (sets of images) that correspond to a single drive, we need to ensure that each clip in its entirety is in either our **train set** or **test set**. This will eliminate any overlapping between the train and test data. Below is each clip in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ec97af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clipNames'] = df[['image_id']].applymap(lambda x: x.split('/')[2])\n",
    "df['clipNames'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75aa1dd5",
   "metadata": {},
   "source": [
    "There exist 13 daytime clips and 5 nighttime clips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a56f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(df,p=0.25):\n",
    "    clipNames = sorted(df['clipNames'].unique())\n",
    "\n",
    "    nightClips = [name for name in clipNames if 'night' in name]\n",
    "    dayClips = [name for name in clipNames if 'day' in name]\n",
    "\n",
    "    testNightClipNames = list(np.random.choice(nightClips,int(len(nightClips)*p)))\n",
    "    testDayClipNames = list(np.random.choice(dayClips,int(len(dayClips)*p)))\n",
    "    testClipNames = testNightClipNames + testDayClipNames\n",
    "\n",
    "    trainDayClipNames = list(set(dayClips) - set(testDayClipNames))\n",
    "    trainNightClipNames = list(set(nightClips) - set(testNightClipNames))\n",
    "    trainClipNames = trainNightClipNames + trainDayClipNames\n",
    "    \n",
    "    train_df = df[df.clipNames.isin(trainClipNames)]\n",
    "    test_df = df[df.clipNames.isin(testClipNames)]\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b130415b",
   "metadata": {},
   "source": [
    "Using the above function, we split our dataframe into a train and test set by placing approximately a quarter of both `nightClips` and `dayClips` into the test set while placing the remainder into the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cbf258",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = split(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c43c073",
   "metadata": {},
   "source": [
    "Here is what our data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940fd9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c3fc74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b87a824",
   "metadata": {},
   "source": [
    "Confirming that about a quarter of our data is in the test dataframe and the remainder is in the train dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3c7d83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Train shape: \",train_df.shape)\n",
    "print(\"Test shape: \",test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5baa4d5",
   "metadata": {},
   "source": [
    "#### Train and Validation Split\n",
    "We further designate data from our train dataframe as our validation dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9699a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = split(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabf9a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0c4a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e32fa5",
   "metadata": {},
   "source": [
    "### 3. Utils\n",
    "Before we can start fine-tuning, we need to prepare a couple of items."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f161706",
   "metadata": {},
   "source": [
    "#### Declare a couple of constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b06c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 3\n",
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a52dfc8",
   "metadata": {},
   "source": [
    "#### Set up our GPU if available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d09073",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb1687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c00d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def report_gpu():\n",
    "    print(torch.cuda.list_gpu_processes())\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6345972",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523928d1",
   "metadata": {},
   "source": [
    "#### Create a custom dataset object:\n",
    "Here, we import our custom traffic lights dataset defined in a separate file. It is structured this way to avoid multithreading issues with `num_workers > 0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68002ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from traffic_lights_dataset import TrafficLightsDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9a5345",
   "metadata": {},
   "source": [
    "#### Average loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2412cd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average loss -> (Total-Loss / Total-Iterations)\n",
    "class LossAverager:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61948a9",
   "metadata": {},
   "source": [
    "#### Collate function to specify batching\n",
    "`collate_fn` is used by PyTorch's `DataLoader` to specify how data from the dataset should be batched. PyTorch's default `DataLoader` will stack our samples in `BATCH_SIZE` tuples. The custom batching defined by this collate function will return a tuple of two lists that is of length `BATCH_SIZE`. This is just to make it easier to separate the images and targets later on. Like our custom traffic lights dataset class, we have defined this function in a separate `collate_fn.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a7e0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Batching with no collate function your batch data would look like:\n",
    "# [(img_0, targets_0), (img_1, targets_1), ...]\n",
    "# but with the collate function it would be more like\n",
    "# [(img_0, img_1), (targets_0, targets_1), ...]\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#    return tuple(zip(*batch))\n",
    "\n",
    "from collate_fn import collate_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeafb82",
   "metadata": {},
   "source": [
    "#### Augmenting the images\n",
    "Augmenting the images using various image transformations such as rotations, translations, zooms, and changes in lighting helps to increase the diversity of the training dataset without gathering more data points. The goal is to prevent the likelihood of our model overfitting. Remember, we imported the albumentations library as `A`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd3b7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Albumentations\n",
    "\n",
    "# For Train Data\n",
    "def getTrainTransform():\n",
    "    return A.Compose([\n",
    "        A.Resize(height=512, width=512, p=1),\n",
    "        A.Flip(0.5),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "\n",
    "# For Validation Data\n",
    "def getValTransform():\n",
    "    return A.Compose([\n",
    "        A.Resize(height=512, width=512, p=1),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "\n",
    "# For Test Data\n",
    "def getTestTransform():\n",
    "    return A.Compose([\n",
    "        A.Resize(height=512, width=512, p=1),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ca45ca",
   "metadata": {},
   "source": [
    "#### Data Loaders using our custom dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8581ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset = TrafficLightsDataset(train_df,getTrainTransform())\n",
    "valDataset = TrafficLightsDataset(val_df,getValTransform())\n",
    "testDataset = TrafficLightsDataset(test_df,getTestTransform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8551d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataLoader = DataLoader(\n",
    "    trainDataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "valDataLoader = DataLoader(\n",
    "    valDataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "testDataLoader = DataLoader(\n",
    "    testDataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777a6115",
   "metadata": {},
   "source": [
    "#### Checking our Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2f715a",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets, image_ids = next(iter(trainDataLoader))\n",
    "\n",
    "boxes = targets[0]['boxes'].cpu().numpy().astype(np.int32)\n",
    "image = images[0].permute(1,2,0).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b98528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayImage(image, boxes):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "\n",
    "    for box in boxes:\n",
    "        cv2.rectangle(image,\n",
    "                      (box[0], box[1]),\n",
    "                      (box[2], box[3]),\n",
    "                      (220, 0, 0), 3)\n",
    "\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bd3f41",
   "metadata": {},
   "source": [
    "Here is an example of one of our augmented images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d61bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "displayImage(image,boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8619aacc",
   "metadata": {},
   "source": [
    "### 4. Model\n",
    "Here, we import from PyTorch's torchvision library FasterRCNN pretrained on the COCO dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9e229a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "N_CLASS = 4  # 3 classes (Stop, Warning, Go) + Background\n",
    "\n",
    "# Number of Input Features for the Classifier Head\n",
    "INP_FEATURES = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# New Head for Classification\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(INP_FEATURES, N_CLASS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e50e0b",
   "metadata": {},
   "source": [
    "### 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80e6857",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4205cdfd-a101-40e0-8938-3432e2bdac3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "# Optimizers\n",
    "optimizer = torch.optim.Adam(params)\n",
    "\n",
    "# LR Scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4bb65616",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "lossHist = LossAverager()\n",
    "valLossHist = LossAverager()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    start_time = time()\n",
    "    model.train()\n",
    "    lossHist.reset()\n",
    "    \n",
    "    for images, targets, image_ids in tqdm(trainDataLoader):\n",
    "        \n",
    "        images = torch.stack(images).to(device)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        bs = images.shape[0]\n",
    "        \n",
    "        loss_dict = model(images, targets)\n",
    "        \n",
    "        totalLoss = sum(loss for loss in loss_dict.values())\n",
    "        lossValue = totalLoss.item()\n",
    "        \n",
    "        lossHist.update(lossValue,bs)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        totalLoss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # LR Update\n",
    "    if lr_scheduler is not None:\n",
    "        lr_scheduler.step(totalLoss)\n",
    "\n",
    "    print(f\"[{str(datetime.timedelta(seconds = time() - start_time))[2:7]}]\")\n",
    "    print(f\"Epoch {epoch}/{EPOCHS}\")\n",
    "    print(f\"Train loss: {lossHist.avg}\")\n",
    "    \n",
    "    torch.save(model.state_dict(), 'fasterrcnn_resnet50_fpn.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65ff3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895901d7-01c0-4f47-a687-64d858e88cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('fasterrcnn_resnet50_fpn.pth', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34659004",
   "metadata": {},
   "source": [
    "### 9. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a8c162-675f-4c54-95b1-f934709251c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "images, targets, image_ids = next(iter(testDataLoader))\n",
    "images = torch.stack(images).to(torch.device('cpu'))\n",
    "\n",
    "outputs = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c035f6a-0d23-48da-9b6c-3cda3d10a440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterBoxes(output,nms_th=0.3,score_threshold=0.5):\n",
    "    \n",
    "    boxes = output['boxes']\n",
    "    scores = output['scores']\n",
    "    labels = output['labels']\n",
    "    \n",
    "    # Non Max Supression\n",
    "    mask = nms(boxes,scores,nms_th)\n",
    "    \n",
    "    boxes = boxes[mask]\n",
    "    scores = scores[mask]\n",
    "    labels = labels[mask]\n",
    "    \n",
    "    boxes = boxes.data.cpu().numpy().astype(np.int32)\n",
    "    scores = scores.data.cpu().numpy()\n",
    "    labels = labels.data.cpu().numpy()\n",
    "    \n",
    "    mask = scores >= score_threshold\n",
    "    boxes = boxes[mask]\n",
    "    scores = scores[mask]\n",
    "    labels = labels[mask]\n",
    "    \n",
    "    return boxes, scores, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1ac1c6-5fdb-4299-bccf-2d35f60222e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayPredictions(image_id,output,nms_th=0.3,score_threshold=0.5):\n",
    "    \n",
    "    boxes,scores,labels = filterBoxes(output,nms_th,score_threshold)\n",
    "    \n",
    "    # Preprocessing\n",
    "    image = cv2.imread(image_id)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "    image = cv2.resize(image,(512,512))\n",
    "    image /= 255.0\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "\n",
    "    colors = {1:(0,255,0), 2:(255,255,0), 3:(255,0,0)}\n",
    "    \n",
    "    for box,label in zip(boxes,labels):\n",
    "        image = cv2.rectangle(image,\n",
    "                      (box[0], box[1]),\n",
    "                      (box[2], box[3]),\n",
    "                      colors[label], 2)\n",
    "\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272bcf09-b929-44d7-b832-6a4bb06eee1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "displayPredictions(image_ids[2],outputs[2],0.2,0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fdd493",
   "metadata": {},
   "source": [
    "### 10. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0209ff-fd6c-4bf7-9652-8a55e7daf085",
   "metadata": {},
   "source": [
    "There are many optimization improvements that can be made to the:\n",
    "- Augmentation techniques\n",
    "- Validation of the FasterRCNN model\n",
    "\n",
    "There are also some changes I would like to make in the future:\n",
    "- Increase the robustness of the model by including go on left and stop on left detection.\n",
    "- Include my own dataset in training and compare it to the results I currently have to see if I can notice a difference (driving in my area).\n",
    "\n",
    "There were also minor hiccups when trying to use the metal performance shaders (MPS) package to train the model on my local machine (which runs Apple silicon). This is simply due to the MPS backend not supporting the `aten::hardsigmoid` operator. This is unfortunate due to the fact that the FasterRCNN architecture relies on this operation. A list of currently unsuported operators can be found in the issues section of the official PyTorch GitHub Repository [here](https://github.com/pytorch/pytorch/issues/77764). As a result, I ended up using a free CUDA GPU on Google Colab, running the training script there, and downloading the model locally."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
